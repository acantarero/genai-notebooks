{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pxo78uuVS26f"
   },
   "source": [
    "# GPT3.5 Instructor & FLARE\n",
    "\n",
    "FLARE is a simple way to improve document Q&A, however, it depends on the logprobs being available for each token.\n",
    "\n",
    "Many foundation models do not return logprobs, so FLARE often relies on older generative models.\n",
    "\n",
    "With the recent launch of `gpt-3.5-turbo-instruct` from OpenAI, we can improve FLARE results.\n",
    "\n",
    "In this notebook, we'll look at\n",
    "\n",
    "- Using a FLARE chain [from langchain](https://python.langchain.com/docs/use_cases/question_answering/how_to/flare) with OpenAI's `gpt-3.5-turbo-instruct` model\n",
    "- Embeddings generated using OpenAI `text-embedding-ada-002`\n",
    "- Astra DB (build on Apache Cassandra) as our vector store\n",
    "\n",
    "Let's take a look."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is FLARE?\n",
    "\n",
    "Forward-Looking Active REtrieval augmented generation ([FLARE](https://arxiv.org/abs/2305.06983)) is an approach to improve the results of text generations from the LLM.\n",
    "\n",
    "It works by starting to answer a question, then as tokens are used that the LLM is uncertain about, FLARE generates new search queries to the vector database to retrieve additional documents to use in the text generation.\n",
    "\n",
    "Below is an example from the original paper:\n",
    "\n",
    "![FLARE example](https://drive.google.com/uc?export=view&id=1pyes2ZoV97zHkyB85oSlYDD56X6Gbvoy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pDOeb2U_2d1k"
   },
   "source": [
    "## Confirm the GPT3.5 Instruct returns logprobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X0klGw4df1YF",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install langchain openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "vOrm-On0gJli"
   },
   "outputs": [],
   "source": [
    "# this code ensure that long text generations for the text generation wrap for\n",
    "# readability\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "def set_css():\n",
    "  display(HTML('''\n",
    "  <style>\n",
    "    pre {\n",
    "        white-space: pre-wrap;\n",
    "    }\n",
    "  </style>\n",
    "  '''))\n",
    "get_ipython().events.register('pre_run_cell', set_css)\n",
    "\n",
    "def set_css():\n",
    "  display(HTML('''\n",
    "  <style>\n",
    "    pre {\n",
    "        white-space: pre-wrap;\n",
    "    }\n",
    "  </style>\n",
    "  '''))\n",
    "get_ipython().events.register('pre_run_cell', set_css)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If you do not have an OpenAI API Key, get one at https://platform.openai.com.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "jUl5dJ-RgMrc",
    "outputId": "e8a505e8-1afc-4ab4-9142-f7c0de4c6a8c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <style>\n",
       "    pre {\n",
       "        white-space: pre-wrap;\n",
       "    }\n",
       "  </style>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "  <style>\n",
       "    pre {\n",
       "        white-space: pre-wrap;\n",
       "    }\n",
       "  </style>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Your OpenAI API key (sk-...) ········\n"
     ]
    }
   ],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "# set your OpenAI API key\n",
    "OPENAI_API_KEY = getpass.getpass(\"Your OpenAI API key (sk-...)\")\n",
    "\n",
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NUcsOM3H264i"
   },
   "source": [
    "Here we see that the new model does return logprobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "dUZ2cy2hpSbb",
    "outputId": "7406ca07-24a6-451e-ea35-1c3240340a3e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <style>\n",
       "    pre {\n",
       "        white-space: pre-wrap;\n",
       "    }\n",
       "  </style>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "  <style>\n",
       "    pre {\n",
       "        white-space: pre-wrap;\n",
       "    }\n",
       "  </style>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alejandro.cantarero/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:34: NotOpenSSLWarning: urllib3 v2.0 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"cmpl-82hPMBao1h4HShRrhwY07jFmNb9nV\",\n",
      "  \"object\": \"text_completion\",\n",
      "  \"created\": 1695654208,\n",
      "  \"model\": \"gpt-3.5-turbo-instruct\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"text\": \"\\nGeorge Washington (1732-1799) was the first president of the United States of America, serving two terms\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": {\n",
      "        \"tokens\": [\n",
      "          \"\\n\",\n",
      "          \"George\",\n",
      "          \" Washington\",\n",
      "          \" (\",\n",
      "          \"173\",\n",
      "          \"2\",\n",
      "          \"-\",\n",
      "          \"179\",\n",
      "          \"9\",\n",
      "          \")\",\n",
      "          \" was\",\n",
      "          \" the\",\n",
      "          \" first\",\n",
      "          \" president\",\n",
      "          \" of\",\n",
      "          \" the\",\n",
      "          \" United\",\n",
      "          \" States\",\n",
      "          \" of\",\n",
      "          \" America\",\n",
      "          \",\",\n",
      "          \" serving\",\n",
      "          \" two\",\n",
      "          \" terms\"\n",
      "        ],\n",
      "        \"token_logprobs\": [\n",
      "          -1.7701586,\n",
      "          -0.0016792956,\n",
      "          -0.00040832703,\n",
      "          -0.4290861,\n",
      "          -0.04477831,\n",
      "          -0.0004197157,\n",
      "          -0.0057247938,\n",
      "          -0.0004609816,\n",
      "          -4.144026e-05,\n",
      "          -0.0004988487,\n",
      "          -0.0020143553,\n",
      "          -2.2260659,\n",
      "          -0.0019142022,\n",
      "          -1.2683872,\n",
      "          -0.00011523515,\n",
      "          -4.7279616e-05,\n",
      "          -9.251094e-06,\n",
      "          -0.00013597934,\n",
      "          -1.2184083,\n",
      "          -0.00024025032,\n",
      "          -0.48995513,\n",
      "          -0.00463459,\n",
      "          -2.6991084,\n",
      "          -0.009324512\n",
      "        ],\n",
      "        \"top_logprobs\": null,\n",
      "        \"text_offset\": [\n",
      "          26,\n",
      "          27,\n",
      "          33,\n",
      "          44,\n",
      "          46,\n",
      "          49,\n",
      "          50,\n",
      "          51,\n",
      "          54,\n",
      "          55,\n",
      "          56,\n",
      "          60,\n",
      "          64,\n",
      "          70,\n",
      "          80,\n",
      "          83,\n",
      "          87,\n",
      "          94,\n",
      "          101,\n",
      "          104,\n",
      "          112,\n",
      "          113,\n",
      "          121,\n",
      "          125\n",
      "        ]\n",
      "      },\n",
      "      \"finish_reason\": \"length\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 6,\n",
      "    \"completion_tokens\": 24,\n",
      "    \"total_tokens\": 30\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "\n",
    "response = openai.Completion.create(\n",
    "  model=\"gpt-3.5-turbo-instruct\",\n",
    "  prompt=\"who was george washington?\",\n",
    "  max_tokens=24,\n",
    "  logprobs=0\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CI77NOY-24HN"
   },
   "source": [
    "## Connect GPT3.5 Instruct to FLARE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 839
    },
    "id": "dzWONFPutooF",
    "outputId": "ed296fb2-48c3-4c64-a921-ec55d5fed56a",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install langchain cassio pypdf tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yspp8vNf3PIx"
   },
   "source": [
    "### Let's connect to Astra DB\n",
    "\n",
    "Sign up for a free account at https://astra.datastax.com.\n",
    "\n",
    "In this section you'll need:\n",
    "\n",
    "- A token JSON file with the connection credentials for your Astra database\n",
    "- A secure connect bundle for your Astra database.\n",
    "\n",
    "You can generate both of these in the Astra UI on the \"Connect\" tab for your database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 74
    },
    "id": "paIfjDXx3RBz",
    "outputId": "e29355b2-cc38-4d81-b8d4-e460e4af7a6e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <style>\n",
       "    pre {\n",
       "        white-space: pre-wrap;\n",
       "    }\n",
       "  </style>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "  <style>\n",
       "    pre {\n",
       "        white-space: pre-wrap;\n",
       "    }\n",
       "  </style>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ASTRA_TOKEN_PATH = \"/Users/alejandro.cantarero/code/datastax/tokens/astra/alejandro.cantarero@datastax.com-token.json\"\n",
    "ASTRA_DB_SECURE_BUNDLE_PATH = \"/Users/alejandro.cantarero/code/datastax/tokens/astra/secure-connect-astra-demo-db.zip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 285
    },
    "id": "HtAJHHv53YaJ",
    "outputId": "a0148c3c-048a-440b-9e00-d841f548a2ec"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <style>\n",
       "    pre {\n",
       "        white-space: pre-wrap;\n",
       "    }\n",
       "  </style>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "  <style>\n",
       "    pre {\n",
       "        white-space: pre-wrap;\n",
       "    }\n",
       "  </style>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Connect to Astra\n",
    "from cassandra.cluster import Cluster\n",
    "from cassandra.auth import PlainTextAuthProvider\n",
    "import json\n",
    "\n",
    "with open(ASTRA_TOKEN_PATH, \"r\") as f:\n",
    "    creds = json.load(f)\n",
    "    ASTRA_DB_APPLICATION_TOKEN = creds[\"token\"]\n",
    "\n",
    "cluster = Cluster(\n",
    "    cloud={\n",
    "        # name needs to match file you uploaded\n",
    "        \"secure_connect_bundle\": ASTRA_DB_SECURE_BUNDLE_PATH,\n",
    "    },\n",
    "    auth_provider=PlainTextAuthProvider(\n",
    "        \"token\",\n",
    "        ASTRA_DB_APPLICATION_TOKEN,\n",
    "    ),\n",
    ")\n",
    "astraSession = cluster.connect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M30q9mLJ3g8T"
   },
   "source": [
    "### Fetch data and create embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "hBYDeCzt3oUl",
    "outputId": "6330d6ee-5ea8-4dfb-c878-566aff83d222"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <style>\n",
       "    pre {\n",
       "        white-space: pre-wrap;\n",
       "    }\n",
       "  </style>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "  <style>\n",
       "    pre {\n",
       "        white-space: pre-wrap;\n",
       "    }\n",
       "  </style>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# set parameters for data embedding\n",
    "keyspace = \"pdf_demo\" # keyspace you made in Astra\n",
    "src_dir = \"data/prompt_papers\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "5xVzf8r93vkA"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <style>\n",
       "    pre {\n",
       "        white-space: pre-wrap;\n",
       "    }\n",
       "  </style>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "  <style>\n",
       "    pre {\n",
       "        white-space: pre-wrap;\n",
       "    }\n",
       "  </style>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<cassandra.cluster.ResultSet at 0x13eac4730>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# optionally drop the table to regenerate the embeddings\n",
    "astraSession.execute(f\"DROP TABLE IF EXISTS {keyspace}.flare_pdf_demo;\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "vdG41fwA3zO9",
    "outputId": "d0153773-77c1-4ff6-ea6d-3dd95c5af5a3"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <style>\n",
       "    pre {\n",
       "        white-space: pre-wrap;\n",
       "    }\n",
       "  </style>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "  <style>\n",
       "    pre {\n",
       "        white-space: pre-wrap;\n",
       "    }\n",
       "  </style>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# set up the vector store\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Cassandra\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "vectorstore = Cassandra(\n",
    "    embedding=embeddings,\n",
    "    session=astraSession,\n",
    "    keyspace=keyspace,\n",
    "    table_name=\"flare_pdf_demo\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uk8OZt3q310a",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# run the embedding process\n",
    "\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "FILE_SUFFIX = \".pdf\"\n",
    "\n",
    "list_of_pdfs = []\n",
    "# generate the list of PDF files\n",
    "for f in os.listdir(src_dir):\n",
    "  filename = os.path.join(src_dir, f)\n",
    "  if os.path.isfile(filename) and f[-len(FILE_SUFFIX):] == FILE_SUFFIX:\n",
    "    list_of_pdfs.append(filename)\n",
    "\n",
    "# tell us what files are being processed\n",
    "print(\"Files found:\")\n",
    "pprint(list_of_pdfs)\n",
    "\n",
    "pdf_loaders = [\n",
    "    PyPDFLoader(pdf_name)\n",
    "    for pdf_name in list_of_pdfs\n",
    "]\n",
    "\n",
    "# strip and load the docs\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=80,\n",
    ")\n",
    "documents = [\n",
    "    doc\n",
    "    for loader in pdf_loaders\n",
    "    for doc in loader.load_and_split(text_splitter=text_splitter)\n",
    "]\n",
    "#\n",
    "texts, metadatas = zip(*((doc.page_content, doc.metadata) for doc in documents))\n",
    "vectorstore.add_texts(texts=texts, metadatas=metadatas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "oUDL7V8i37O2",
    "outputId": "23ec64bd-4a5a-47d9-efbf-24645283fed1"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <style>\n",
       "    pre {\n",
       "        white-space: pre-wrap;\n",
       "    }\n",
       "  </style>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "  <style>\n",
       "    pre {\n",
       "        white-space: pre-wrap;\n",
       "    }\n",
       "  </style>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(count=2919)\n"
     ]
    }
   ],
   "source": [
    "# check how much data we loaded\n",
    "# NOTE: count() is not a good way to do this and will timeout for large tables\n",
    "#       but is an easy approach for a small number of documents\n",
    "row = astraSession.execute(\"select count(*) from pdf_demo.flare_pdf_demo;\")\n",
    "print(row.one())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "FWI5qQC2Wh_b",
    "outputId": "8a1e8ec8-603b-4c02-ee23-fe43de51174b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <style>\n",
       "    pre {\n",
       "        white-space: pre-wrap;\n",
       "    }\n",
       "  </style>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "  <style>\n",
       "    pre {\n",
       "        white-space: pre-wrap;\n",
       "    }\n",
       "  </style>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# define a function to extract FLARE generated questions from the verbose langchain logs\n",
    "import re\n",
    "\n",
    "def get_generated_questions(logs):\n",
    "  pattern = r\"Generated Questions:\\s*\\[(.*?)\\]\"\n",
    "  matches = re.findall(pattern, logs)\n",
    "\n",
    "  if matches:\n",
    "    questions = matches[0].split(\", \")\n",
    "    return questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that langchain does not provide a standard interface for changing out the generative model used in the `FlareChain`. Because of this, the way you create the `FlareChain` with the `gpt-3.5-turbo-instruct` model is different than the standard interface.\n",
    "\n",
    "This will likely be updated by langchain in the future so they share a common interface where you can change models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 231
    },
    "id": "mMBkFG-c37oP",
    "outputId": "3cc81c74-070c-42f3-e43d-39bc10b9c61b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <style>\n",
       "    pre {\n",
       "        white-space: pre-wrap;\n",
       "    }\n",
       "  </style>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "  <style>\n",
       "    pre {\n",
       "        white-space: pre-wrap;\n",
       "    }\n",
       "  </style>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GENERATED FLARE QUESTIONS:\n",
      "['\" How can you fix your chatbot\\'s incorrect instructions on how to perform tasks in the UI?\"', \"' What are some ways to adjust the prompting techniques used by your chatbot?'\"]\n",
      "\n",
      "\n",
      "FLARE RESULT:\n",
      "  You can use a variety of prompting techniques to help your chatbot give more accurate instructions. For example, you can use multiple-rounds of few-shot prompting (denoted as “composite prompting”) to combine different formats of prompts for the same task. You can also use prompt tuning, which involves fine-tuning the prompts to better match the task. Finally, you can use in-domain examples taken from the GSM8K train set to engineer the prompts. \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "-------------------\n",
      "\n",
      "FLARE RESULT 3.5:\n",
      " You can try using decomposition prompts or least-to-most prompting to help your chatbot better understand the task and provide accurate instructions.  Additionally, you can also use errorless learning techniques to prevent your chatbot from giving incorrect instructions in the future. \n",
      "\n",
      "\n",
      "GENERATED FLARE QUESTIONS (with GPT3.5):\n",
      "[\"' What is one possible solution to fix the issue with incorrect instructions from the chatbot?'\", \"' What is one possible solution to fix the issue with incorrect instructions from the chatbot?'\", \"' What can you do to improve the clarity of your instructions?'\", \"' What can using more specific and detailed prompts help with when giving instructions?'\", \"' What can you do to ensure that the user understands the correct steps to perform the task?'\"]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import langchain\n",
    "from langchain.chains import FlareChain\n",
    "from langchain.chains.flare.base import QuestionGeneratorChain, _OpenAIResponseChain\n",
    "from langchain.llms import OpenAI\n",
    "from io import StringIO\n",
    "import sys\n",
    "\n",
    "langchain.verbose = False\n",
    "\n",
    "#query = \"how should I write a prompt to solve a math problem?\"\n",
    "#query = \"my prompt produced SQL code that is incorrect. how can I fix it?\"\n",
    "query = \"my chatbot is giving incorrect instructions on how to perform tasks in the UI. What prompting techniques can I use to fix this?\"\n",
    "\n",
    "langchain.verbose = True\n",
    "\n",
    "# retrieve data from the vector store created above\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# setup a default LLM with no FLARE for comparison\n",
    "llm = OpenAI(temperature=0)\n",
    "\n",
    "original_stdout = sys.stdout\n",
    "stdout_str = StringIO()\n",
    "sys.stdout = stdout_str\n",
    "\n",
    "# create the FLARE process\n",
    "flare = FlareChain.from_llm(\n",
    "    llm,\n",
    "    retriever=retriever,\n",
    "    max_generation_len=512,\n",
    "    min_prob=0.3,\n",
    ")\n",
    "\n",
    "# setup new instruct model\n",
    "llm35 = OpenAI(\n",
    "    model=\"gpt-3.5-turbo-instruct\",\n",
    "    temperature=0,\n",
    "    max_tokens=512,\n",
    "    model_kwargs={\"logprobs\": 1}\n",
    ")\n",
    "\n",
    "flare35 = FlareChain(\n",
    "    question_generator_chain=QuestionGeneratorChain(llm=llm35),\n",
    "    response_chain=_OpenAIResponseChain(llm=llm35),\n",
    "    retriever=retriever,\n",
    "    min_prob=0.3,\n",
    ")\n",
    "\n",
    "flare_result = flare.run(query)\n",
    "questions = get_generated_questions(stdout_str.getvalue())\n",
    "\n",
    "stdout_str.truncate(0)\n",
    "stdout_str.seek(0)\n",
    "\n",
    "flare35_result = flare35.run(query)\n",
    "questions35 = get_generated_questions(stdout_str.getvalue())\n",
    "\n",
    "sys.stdout = original_stdout\n",
    "\n",
    "# call model directly\n",
    "llm_response = openai.Completion.create(\n",
    "  model=\"gpt-3.5-turbo-instruct\",\n",
    "  prompt=query,\n",
    "  max_tokens=512,\n",
    ")\n",
    "\n",
    "print(f\"GENERATED FLARE QUESTIONS:\\n{questions}\\n\\n\")\n",
    "print(f\"FLARE RESULT:\\n {flare_result}\\n\\n\")\n",
    "\n",
    "print(\"-------------------\\n\")\n",
    "print(f\"GENERATED FLARE QUESTIONS (with GPT3.5):\\n{questions35}\\n\\n\")\n",
    "print(f\"FLARE RESULT 3.5:\\n {flare35_result}\\n\\n\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
